# ğŸ” PrivateGPT Local Deployment â€“ LlamaCPP (WSL)

## ğŸ“Œ Project Overview

This project demonstrates the secure local deployment of PrivateGPT using:

- ğŸ§  Meta-Llama-3.1-8B (GGUF)
- âš™ï¸ LlamaCPP backend
- ğŸ§ WSL (Ubuntu)
- ğŸ’» Windows 11 Host
- ğŸ“¦ Offline document ingestion (RAG pipeline)

The system runs fully offline.  
No cloud APIs. No external data exposure.

---

## ğŸ›¡ Security Objectives

- All inference runs locally
- Bound to 127.0.0.1 (no external exposure)
- Model files excluded from Git tracking
- Secrets removed from configuration files
- GitHub workflows removed to reduce CI attack surface
- Token-based Git authentication (PAT)

---

## ğŸ— Architecture

Windows 11 (Host)
â”‚
â””â”€â”€ WSL Ubuntu
    â”œâ”€â”€ Python 3.11 (Poetry)
    â”œâ”€â”€ PrivateGPT
    â”œâ”€â”€ LlamaCPP
    â””â”€â”€ GGUF Model (Local)

Accessed via local Gradio UI (localhost only).

---

## âš™ï¸ Deployment Summary

1. Installed WSL on Windows 11  
2. Cloned PrivateGPT repository  
3. Installed LlamaCPP dependencies  
4. Downloaded Meta-Llama-3.1-8B GGUF model  
5. Configured `settings-llamacpp.yaml`  
6. Launched locally via:

```bash
poetry run python -m private_gpt
```

---

## ğŸ¯ Cybersecurity Relevance

This project demonstrates:

- Secure AI deployment
- Offline RAG architecture
- Environment isolation using WSL
- Model integrity handling
- Secure Git configuration
- Secret management best practices

---

## ğŸ‘¤ Author

Deloney Sime  
Cybersecurity Engineer | CEH | AI Security
